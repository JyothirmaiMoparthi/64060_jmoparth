---
title: "FML ASSIGNMENT2"
author: "JYOTHIRMAI MOPARTHI"
date: "2023-09-14"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#QUESTION
#summary1. Age = 40, Experience = 10, Income = 84, Family = 2, CCAvg = 2, Education_1 = 0, Education_2 =1, Education_3 = 0, Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1, andCredit Card = 1. Perform a k-NN classification with all predictors except ID and ZIP codeusing k = 1. Remember to transform categorical predictors with more than two categoriesinto dummy variables first. Specify the success class as 1 (loan acceptance), and use thedefault cutoff value of 0.5. How would this customer be classified?

#2. What is a choice of k that balances between overfitting and ignoring the predictor information?

#3. Show the confusion matrix for the validation data that results from using the best k.

#4. Consider the following customer: Age = 40, Experience = 10, Income = 84,Family = 2, CCAvg = 2, Education_1 = 0, Education_2 = 1, Education_3 = 0,Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1 and CreditCard = 1. Classify the customer using the best k.

#5Repartition the data, this time into training, validation, and test sets (50% : 30% : 20%). Apply the k-NN method with the k chosen above. Compare the confusion matrix of the test set with that of the training and validation sets. Comment on the differences and their reason.

# load required libraries
```{r}
library(class)
library(caret)
library(e1071)
```
# Data Cleaning
```{r}
universal.df <- read.csv("./universalBank.csv")
dim(universal.df)
t(t(names(universal.df))) # The t function creates a transpose of the data frame
```
# Drop variables ID and Zip
```{r}
universal.df <- universal.df[,-c(1,5)]
```
# Education needs to be convertd to factor
```{r}
universal.df$Education <- as.factor(universal.df$Education)
```
# Converting Education to Dummy Variables
```{r}
groups <- dummyVars(~. , data=universal.df) #this creates dummy groups
```
```{r}
universal_m.df <- as.data.frame(predict(groups,universal.df))
```
```{r}
set.seed(1)
#Important to ensure that we get the same sample if we rerun the code
train.index <- sample(row.names(universal_m.df),0.6*dim(universal_m.df)[1])
valid.index <- setdiff(row.names(universal_m.df),train.index)
train.df <- universal_m.df[train.index,]
valid.df <- universal_m.df[valid.index,]
t(t(names(train.df)))
```
# Now normalizing the data
```{r}
train.norm.df <- train.df[,-10]
#Note that personal income is the 10th variable
valid.norm.df <- valid.df[,-10]
norm.values <- preProcess(train.df[,-10],method=c("center","scale"))
train.norm.df <- predict(norm.values,train.df[,-10])
valid.norm.df <- predict(norm.values,valid.df[,-10])
```
```{r}
#we have converted all categorical variables to dummy variables
#let's create a new sample
#Question 1

new_customer <- data.frame(
  Age = 40,
  Experience = 10,
  Income = 84,
  Family = 2,
  CCAvg = 2,
  Education.1 = 0,
  Education.2 = 1,
  Education.3 = 0,
  Mortgage = 0,
  Securities.Account = 0,
  CD.Account = 0,
  Online = 1,
  CreditCard = 1)
```

# Normalize the new customer

```{r}
new.cust.norm <- new_customer
new.cust.norm <- predict(norm.values, new.cust.norm)

```

# Now let us predict using K-NN(k- Nearest neighbors)

```{r}

knn.pred1 <- class::knn(train = train.norm.df, 
                       test = new.cust.norm, 
                       cl = train.df$Personal.Loan, k = 1)
knn.pred1

```
# Calculate the accuracy for each value of k    
# Set the range of k values to consider
#Question 2
```{r}

accuracy.df <- data.frame(k = seq(1, 15, 1), overallaccuracy = rep(0, 15)) 
for(i in 1:15) 
  {knn.pred <- class::knn(train = train.norm.df, 
                         test = valid.norm.df, 
                         cl = train.df$Personal.Loan, k = i)
  accuracy.df[i, 2] <- confusionMatrix(knn.pred,as.factor(valid.df$Personal.Loan),
                                       positive = "1")$overall[1]
}
which(accuracy.df[,2] == max(accuracy.df[,2])) 
plot(accuracy.df$k,accuracy.df$overallaccuracy, main = "Accuracy Vs K", 
     xlab = "K", ylab = "Accuracy", col= "Red")

```



# Question 3
# Confusion Matrix using best K=3

```{r}

knn.predict <- class::knn(train = train.norm.df,
                         test = valid.norm.df, 
                         cl = train.df$Personal.Loan, k = 3)

confusionMatrix(knn.predict,as.factor(valid.df$Personal.Loan))

```
# Question 4
# Load new customer profile 2

```{r}
new_customer2<-data.frame(
  Age = 40, 
  Experience = 10, 
  Income = 84, 
  family =2, 
  CCAvg = 2, 
  Education_1 = 0,
  Education_2 = 1, 
  Education_3 = 0, 
  Mortgage = 0, 
  Securities.Account = 0, 
  CDAccount = 0, 
  Online = 1, 
  CreditCard = 1)

```


```{r}

knn.predict1 <- class::knn(train = train.norm.df, 
                       test = new.cust.norm, 
                       cl = train.df$Personal.Loan, k = 3)
knn.predict1
```

# Print the predicted class, 1 for loan acceptance and 0 for loan rejection)

```{r}

print("This customer is classified as: Loan Rejected")

```
# Question5
# Split the data into 50% training, 30% Validation and 20% Testing 

```{r}
set.seed(1)
Train_Index1 <- sample(row.names(universal_m.df), 0.5*dim(universal_m.df)[1])
Val_Index1 <- sample(setdiff(row.names(universal_m.df),Train_Index1),0.3*dim(universal_m.df)[1])
Test_Index1 <-setdiff(row.names(universal_m.df),union(Train_Index1,Val_Index1))
Train_Data <- universal_m.df[Train_Index1,]
Validation_Data <- universal_m.df[Val_Index1,]
Test_Data <- universal_m.df[Test_Index1,]
```

# Normalizing the data

```{r}
train.norm.df1 <- Train_Data[,-10]
valid.norm.df1 <- Validation_Data[,-10]
Test.norm.df1  <-Test_Data[,-10]

norm.values1 <- preProcess(Train_Data[, -10], method=c("center", "scale"))
train.norm.df1 <- predict(norm.values1, Train_Data[,-10])
valid.norm.df1 <- predict(norm.values1, Validation_Data[,-10])
Test.norm.df1 <-predict(norm.values1,Test_Data[,-10])

```

# Let us predict using K-NN(k- Nearest neighbors)

```{r}

Validation_knn = class::knn(train = train.norm.df1, 
                           test = valid.norm.df1,  
                           cl = Train_Data$Personal.Loan, 
                           k = 3)

Test_knn = class::knn(train = train.norm.df1, 
                     test = Test.norm.df1,  
                     cl = Train_Data$Personal.Loan, 
                     k = 3)

Train_knn = class::knn(train = train.norm.df1, 
                     test = train.norm.df1,  
                     cl = Train_Data$Personal.Loan, 
                     k = 3)
```

# Validating confusion Matrix

```{r}
validation_confusion_mat = confusionMatrix(Validation_knn, 
                                               as.factor(Validation_Data$Personal.Loan), 
                                               positive = "1")

validation_confusion_mat

```
# Test confusion Matrix

```{r}

Test_confusion_mat = confusionMatrix(Test_knn, 
                                         as.factor(Test_Data$Personal.Loan), 
                                         positive = "1")


Test_confusion_mat

```

```{r}

Training_confusion_mat = confusionMatrix(Train_knn, 
                                               as.factor(Train_Data$Personal.Loan), 
                                               positive = "1")

Training_confusion_mat
```


# Difference

Test vs. Training

***Accuracy:*** When compared to Test, Train is more accurate (0.9772) than it is (0.9507).

**Reason:** This is as a result of variations in the evaluation datasets. The dataset for Train might be better balanced or predictible.

***Sensitivity (True Positive Rate):*** Train has higher sensitivity (0.7589) compared to Test (0.5875).

**Reason:** This indicates that Train's approach is more accurate at spotting positive cases (such as loan approvals). It might have lowered the number of false negatives.

***Specificity (True Negative Rate):*** Train has higher specificity (0.9987) compared to Test (0.99403).

**Reason:** This suggests that Train's model performs better at accurately identifying negative cases (such as loan rejections). It might have a lower false positive rate.

***Positive Predictive Value (Precision):*** Train has a higher positive predictive value (0.9827) compared to Test (0.92157).

**Reason:** Train's model is more precise in predicting positive cases, resulting in fewer false positive predictions.

Train against Validation:

**Accuracy:** Train still exceeds Validation (0.958) in terms of accuracy.

***Reason:*** Similar to the contrast with Test, the dataset for Train might be more evenly distributed or simpler to forecast.

**Sensitivity (True Positive Rate):** Train has higher sensitivity (0.7589) compared to Validation (0.625).

***Reason:*** Train's model is better at correctly identifying positive cases. This indicates that Validation's model may have a higher false negative rate.

**Specificity (True Negative Rate):** Train has higher specificity (0.9987) compared to Validation (0.9934).

***Reason:*** Train's model is better at correctly identifying negative cases. Validation's model may have a slightly higher false positive rate.

Positive Predictive Value (Precision): Train still exceeds Validation (0.9091) in terms of positive predictive value (0.9827).

***Reason:*** Train's model is more precise in predicting positive cases, resulting in fewer false positive predictions.

## Potential Reasons for Differences:

**Data set Differences** Variations in the composition and distribution of data between different sets can significantly impact model performance. For  illustration, one data set may be more imbalanced, making it harder to  prognosticate rare events.   

**Model Variability** Differences in model configurations or  orbitrary initialization of model parameters can lead to variations in performance.

**Hyper parameter Tuning** different hyper parameter settings,similar as the choice of k in k- NN or other model-specific parameters, can affect model performance.

**Data unlocking** If the data sets are  resolve else into training,  confirmation, and test sets in each evaluation, this can lead to variations in results, especially for small data sets.

**Sample Variability** In small data sets, variations in the specific samples included in the  confirmation and test sets can  impact performance  criteria .

**Randomness** Some models,  similar as neural networks, involve randomness in their optimization process, leading to slight variations. 